{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import dask.dataframe as dd  # Efficient handling of large data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "CSVs = [f\"{year}.csv\" for year in range(2000, 2005)]\n",
    "\n",
    "for CSV in CSVs:\n",
    "    # if not os.path.exists(CSV):\n",
    "    #     print(f\"{CSV} not found! Skipping...\")\n",
    "    #     continue  # Skip to the next file\n",
    "\n",
    "    try:\n",
    "        print(f\"Reading {CSV}...\")\n",
    "\n",
    "        # Try reading with UTF-8 first\n",
    "        data = pd.read_csv(CSV, encoding=\"utf-8\")\n",
    "    \n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error decoding {CSV} with UTF-8, trying Latin-1...\")\n",
    "        data = pd.read_csv(CSV, encoding=\"latin1\")\n",
    "\n",
    "    # Keep only required columns\n",
    "    Columns_Needed = [\"Year\", \"DayOfWeek\", \"TailNum\", \"ArrDelay\",\"ArrTime\",\"CRSArrTime\" ,\"DepDelay\",\"DepTime\" ,\"CRSDepTime\"]\n",
    "    Updated_data = data[Columns_Needed]\n",
    "    # Drop missing values\n",
    "    Updated_data = Updated_data.dropna()\n",
    "\n",
    "    # Create new filename\n",
    "    NewCSV = CSV.replace(\".csv\", \"_Q2AB.csv\")\n",
    "    \n",
    "    # Save cleaned data\n",
    "    Updated_data.to_csv(NewCSV, index=False)\n",
    "\n",
    "    print(f\"Processed: {CSV} â†’ Saved as {NewCSV}\")\n",
    "\n",
    "\n",
    "years = range(2000, 2005)\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "data_dict = {}\n",
    "\n",
    "# Read each CSV file\n",
    "for year in years:\n",
    "    file_name = f\"{year}_Q2AB.csv\"  \n",
    "    # Check if file exists\n",
    "    if os.path.exists(file_name): \n",
    "        print(f\"Loading {file_name}...\")\n",
    "        data_dict[year] = pd.read_csv(file_name)\n",
    "    else:\n",
    "        print(f\"File {file_name} not found!\")\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "if data_dict:\n",
    "    combined_data = pd.concat(data_dict.values(), ignore_index=True)\n",
    "    print(\"All files successfully loaded and combined!\")\n",
    "else:\n",
    "    print(\"No files found.\")\n",
    "\n",
    "\n",
    " # Create new CSV file for combined data from 2000 to 2004\n",
    "combined_data.to_csv(\"Combined_Flight_Data_2000_2004.csv\", index=False)\n",
    "print(\"Combined data saved as 'Combined_Flight_Data_2000_2004.csv'\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Combined_Flight_Data_2000_2004.csv\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "df_Q2ab = df[['Year','CRSDepTime', 'DepDelay', 'ArrDelay','DayOfWeek']]\n",
    "day_mapping = {\n",
    "    1: \"Monday\",\n",
    "    2: \"Tuesday\",\n",
    "    3: \"Wednesday\",\n",
    "    4: \"Thursday\",\n",
    "    5: \"Friday\",\n",
    "    6: \"Saturday\",\n",
    "    7: \"Sunday\"\n",
    "}\n",
    "# Drop duplicates \n",
    "\n",
    "df_Q2ab=df_Q2ab.drop_duplicates()\n",
    "\n",
    "# convert day of week from numbers to names\n",
    "df_Q2ab[\"DayOfWeek\"] = df_Q2ab[\"DayOfWeek\"].map(day_mapping)\n",
    "#Display only whole numbers\n",
    "pd.options.display.float_format = '{:g}'.format\n",
    "df_Q2ab['CRSDepTime'] = df_Q2ab['CRSDepTime'].apply(lambda x: '{0:04}'.format(int(x)))\n",
    "df_Q2ab['CRSDepTime'] = pd.to_datetime(df_Q2ab['CRSDepTime'], format='%H%M', errors='coerce')\n",
    "#Time_period['CRSDepTime'] = pd.to_datetime(Time_period['CRSDepTime'], errors='coerce')\n",
    "df_Q2ab['CRSDepTime'] = df_Q2ab['CRSDepTime'].dt.strftime('%H:%M')\n",
    "\n",
    "print(f\"Number of duplicate rows: {df_Q2ab.duplicated().sum()}\")\n",
    "print(df_Q2ab.head(5))\n",
    "\n",
    "#calculate total delay\n",
    "df_Q2ab['TotalDelay'] = df_Q2ab['DepDelay'] + df_Q2ab['ArrDelay']\n",
    "df_Q2ab = df_Q2ab[df_Q2ab['TotalDelay'] >= 0]\n",
    "print(df_Q2ab)\n",
    "\n",
    "# time group of 4 hour interval\n",
    "def categorize_time_groups(depart_time, Time_Interval_minutes):  \n",
    "    time_groups = pd.cut(                              \n",
    "        pd.to_datetime(depart_time).dt.hour * 60 + pd.to_datetime(depart_time).dt.minute,\n",
    "        bins=range(0, 1440, Time_Interval_minutes),  \n",
    "        labels=False\n",
    "    )\n",
    "    return time_groups + 1  \n",
    "\n",
    "Time_Interval_minutes = 239 \n",
    "\n",
    "df_Q2ab['TimeGroup'] = categorize_time_groups(df_Q2ab['CRSDepTime'], Time_Interval_minutes)\n",
    "\n",
    "df_Q2ab = df_Q2ab.dropna()\n",
    "\n",
    "print(df_Q2ab)\n",
    "\n",
    "#generate time interval of 4 hours\n",
    "def generate_time_intervals(time_group):\n",
    "    start_hour = int((time_group - 1) * 4)\n",
    "    end_hour = start_hour + 3\n",
    "    return f'{start_hour:02d}00hrs-{end_hour:02d}59hrs'\n",
    "\n",
    "df_Q2ab['TimeInterval'] = df_Q2ab['TimeGroup'].apply(generate_time_intervals)\n",
    "\n",
    "print(df_Q2ab.head(5))\n",
    "\n",
    "# df_Q2ab = df_Q2ab.sort_values(by=['Year', 'TimeGroup'])\n",
    "\n",
    "# Find mean using groupby in pandas\n",
    "TODMean = df_Q2ab.groupby(['TimeInterval', 'Year'])['TotalDelay'].mean().reset_index()\n",
    "\n",
    "# Rename columns\n",
    "TODMean = TODMean.rename(columns={'TimeInterval': 'Time_Interval', 'Year': 'Year', 'TotalDelay': 'Mean'})\n",
    "pd.options.display.float_format = '{:g}'.format\n",
    "# TODMean['Mean'] = TODMean['Mean'].map(lambda x: f\"{x:.0f}\")\n",
    "print(TODMean.dtypes)\n",
    "print(TODMean.head(5)) \n",
    "print(df_Q2ab.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
